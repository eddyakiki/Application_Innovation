{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661519b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958c3d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "tokenizer = nltk.RegexpTokenizer(r'''\\w'|\\w+|[^\\w\\s]''')\n",
    "import seaborn as sns\n",
    "import time\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import random\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Dense,Dropout,Embedding,LSTM\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "random.seed(123)\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cce9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d3f4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"A.csv\")\n",
    "df_dev = pd.read_csv(\"A.csv\")\n",
    "df_test = pd.read_csv(\"A.csv\")\n",
    "# pour appliquer MLP sur Les données B\n",
    "# df_train = pd.read_csv(\"B.csv\")\n",
    "# df_dev = pd.read_csv(\"B.csv\")\n",
    "# df_test = pd.read_csv(\"B.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3730c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d830f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb44bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_comments = df_train['commentaire'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26706092",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_data = [TaggedDocument(words=word_tokenize(_d), tags=[str(i)]) for i, _d in enumerate(train_comments)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d74ed78",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 10\n",
    "vec_size = 100\n",
    "alpha = 0.03\n",
    "\n",
    "model = Doc2Vec(vector_size=vec_size,\n",
    "                alpha=alpha, \n",
    "                min_alpha=0.00025,\n",
    "                min_count=1,\n",
    "                dm =1)\n",
    "  \n",
    "model.build_vocab(tagged_data)\n",
    "\n",
    "for epoch in range(1,max_epochs+1):\n",
    "#     if epoch%2==0:\n",
    "    print('iteration {0}'.format(epoch))\n",
    "    model.train(tagged_data,\n",
    "                total_examples=model.corpus_count,\n",
    "                epochs=max_epochs)\n",
    "    # decrease the learning rate\n",
    "    model.alpha -= 0.0002\n",
    "    # fix the learning rate, no decay\n",
    "    model.min_alpha = model.alpha\n",
    "\n",
    "model.save(\"d2v_lstm.model\")\n",
    "print(\"Model Saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54baac8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import get_tmpfile\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "model = Doc2Vec.load(\"d2v_lstm.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ceefeea",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dev = []\n",
    "for i in range(len(df_dev)):\n",
    "    vector = model.infer_vector(word_tokenize(df_dev['commentaire'][i])) \n",
    "    emb_dev.append(vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a416d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['doc2vec'] = [model.dv[i] for i in range(len(df_train))]\n",
    "df_dev['doc2vec'] = [vector for vector in emb_dev]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d4fabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train['doc2vec']\n",
    "X_dev = df_dev['doc2vec']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e14678",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = df_train['note']\n",
    "y_dev = df_dev['note']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd5c8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.asarray(X_train.tolist())\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "# x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2785b6c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c723c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_dev = np.asarray(X_dev.tolist())\n",
    "x_dev = np.expand_dims(x_dev, -1)\n",
    "# x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10058f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_dev.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cee6463",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9046995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model = Sequential()\n",
    "model.add(tf.keras.layers.Flatten(input_shape=(100,1)))\n",
    "model.add(Dense(256,activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(256,activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fc829a",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_train=y_train.values-1\n",
    "y_target_train=to_categorical(target_train, num_classes=10)\n",
    "num_classes=y_target_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711c87ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dev=y_dev.values-1\n",
    "y_target_dev=to_categorical(target_dev, num_classes=10)\n",
    "num_classes=y_target_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d0a0f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c589c134",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "st = time.time()\n",
    "\n",
    "#create callback\n",
    "filepath = 'my_best_model_mlp30.hdf5'\n",
    "checkpoint = ModelCheckpoint(filepath=filepath, \n",
    "                             monitor='val_accuracy',\n",
    "                             verbose=1, \n",
    "                             save_best_only=True,\n",
    "                             mode='max')\n",
    "callbacks = [checkpoint]\n",
    "\n",
    "\n",
    "history=model.fit(x_train, y_target_train, validation_data=(x_dev, y_target_dev),epochs=30, batch_size=128, verbose=1,callbacks=callbacks)\n",
    "\n",
    "elapsed_time = time.time() - st\n",
    "print('Execution time:', time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d13dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fc9894",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_test = []\n",
    "for i in range(len(df_dev)):\n",
    "    vector = model.infer_vector(word_tokenize(df_test['commentaire'][i])) \n",
    "    emb_test.append(vector)\n",
    "df_test['doc2vec'] = [vector for vector in emb_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d368205a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = df_test['doc2vec']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e514c911",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('my_best_model_mlp30.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f450349",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.asarray(X_test)\n",
    "x_test = np.expand_dims(x_test, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180d9495",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c16e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7aa66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae6e47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = np.argmax(yhat,axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c62caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648f8a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "notes = [] #génération du fichier pour la plateforme d'évaluation\n",
    "with open(\"sortie_my_best_model_mlp30_doc2vec.txt\", \"w\") as f:\n",
    "    for i in range(len(df_test['review_id'])):\n",
    "        notes.append((prediction[i]+1)/2)\n",
    "        f.write(str(df_test['review_id'].iloc[i])+' '+str(notes[i]).replace('.',',')+'\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
